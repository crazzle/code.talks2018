{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From interactive programming to production ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luigi.contrib.external_program import ExternalProgramTask\n",
    "from luigi.parameter import IntParameter, Parameter\n",
    "from luigi import LocalTarget, Task\n",
    "from helper.keras_util import build_generator\n",
    "from helper.cv2_util import calc_baseline_acc\n",
    "from helper.model_util import define_model\n",
    "import json\n",
    "from keras.models import load_model\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.1: Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to download the dataset using \"curl\".\n",
    "Luigi provides a baseclass named **ExternalProgramTask** to utilize external programs. \n",
    "It simply calls the external program with the provided commandline arguments. The output target can be referenced through *self.output()*.\n",
    "\n",
    "*Input*: Nothing required <br>\n",
    "*Output*: Downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadDataset(ExternalProgramTask):\n",
    "\n",
    "    dataset_version = IntParameter(default=1)\n",
    "    dataset_name = Parameter(default=\"dataset\")\n",
    "\n",
    "    base_url = \"http://plainpixels.work/resources/datasets\"\n",
    "    file_fomat = \"zip\"\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"/tmp/%s_v%d.%s\" % (self.dataset_name,\n",
    "                                               self.dataset_version,\n",
    "                                               self.file_fomat))\n",
    "\n",
    "    def program_args(self):\n",
    "        url = \"%s/%s_v%d.%s\" % (self.base_url, \n",
    "                                self.dataset_name, \n",
    "                                self.dataset_version,\n",
    "                                self.file_fomat)\n",
    "        return [\"curl\", \"-L\",\n",
    "                \"-o\", self.output().path,\n",
    "                url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.2: Extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we use **ExternalProgramTask** to unzip the archive. The major difference is that **ExtractDataset** now implements *requires(...)* and links to **DownloadDataset** as a dependency. The required target can be referenced through *self.input()*.\n",
    "\n",
    "*Input*: DownloadDataset <br>\n",
    "*Output*: A folder containing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractDataset(ExternalProgramTask):\n",
    "    \n",
    "    dataset_version = IntParameter(default=1)\n",
    "    dataset_name = Parameter(default=\"dataset\")\n",
    "    \n",
    "    def requires(self):\n",
    "        return DownloadDataset(self.dataset_version, self.dataset_name)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"datasets/fruit-images-dataset/%d\" % self.dataset_version)\n",
    "\n",
    "    def program_args(self):\n",
    "        self.output().makedirs()\n",
    "        return [\"unzip\", \"-u\", \"-q\",\n",
    "                \"-d\", self.output().path,\n",
    "                self.input().path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.3: Create a preprocessing configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration for the deep-learning model is essentially the Keras ImageDataGenerator. For the sake of simplicity we do not parameterize this task. But we can grasp the idea how to do it.\n",
    "\n",
    "*Input*: Nothing required <br>\n",
    "*Output*: A pickled ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configure(Task):\n",
    "    \n",
    "    config_name = Parameter(default=\"standard\")\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"configurations/%s.pickle\" % self.config_name)\n",
    "\n",
    "    def run(self):\n",
    "        import pickle\n",
    "        from tensorflow import keras\n",
    "        self.output().makedirs()\n",
    "        generator = keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "        with self.output().open(\"w\") as f:\n",
    "            pickle.dump(generator, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.4: Run the baseline validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task runs the baseline validation and saves it to a file. The same as before, flexibility can be greatly enhanced by als versioning the baseline validation.\n",
    "\n",
    "*Input*: ExtractDataset, Configure <br>\n",
    "*Output*: A JSON-File containing the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineValidation(Task):\n",
    "    \n",
    "    dataset_version = IntParameter(default=1)\n",
    "    dataset_name = Parameter(default=\"dataset\")\n",
    "    config_name = Parameter(default=\"standard\")\n",
    "\n",
    "    validation_set = \"Test\"\n",
    "    img_height = 100\n",
    "    img_width = 100\n",
    "    baseline_name = \"find_round_objects.json\"\n",
    "\n",
    "    def requires(self):\n",
    "        yield ExtractDataset(self.dataset_version, self.dataset_name)\n",
    "        yield Configure(self.config_name)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"baseline/%s.json\" % self.baseline_name)\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.input()[0].path\n",
    "        config = self.input()[1].path\n",
    "        test_data = build_generator(config, dataset, self.validation_set)\n",
    "        result = calc_baseline_acc(test_data, dataset, self.validation_set)\n",
    "        with self.output().open(\"w\") as f:\n",
    "            json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.5: Train the deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task No.5 trains a Keras model and persists it to the filesystem.\n",
    "\n",
    "*Input*: ExtractDataset, Configure <br>\n",
    "*Output*: A .h5 file representing the model architecture and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(Task):\n",
    "    \n",
    "    dataset_version = IntParameter(default=1)\n",
    "    dataset_name = Parameter(default=\"dataset\")\n",
    "    config_name = Parameter(default=\"standard\")\n",
    "    model_version = IntParameter(default=1)\n",
    "    model_name = Parameter(default=\"keras_model\")\n",
    "    \n",
    "    training_set = \"Training\"\n",
    "    epochs = 8\n",
    "\n",
    "    def requires(self):\n",
    "        yield ExtractDataset(self.dataset_version, self.dataset_name)\n",
    "        yield Configure(self.config_name)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"model/%d/%s.h5\" % (self.model_version, self.model_name))\n",
    "\n",
    "    def run(self):\n",
    "        self.output().makedirs()\n",
    "        dataset = self.input()[0].path\n",
    "        config = self.input()[1].path\n",
    "        training_data = build_generator(config, dataset, self.training_set)\n",
    "        input_shape = training_data.image_shape\n",
    "        num_classes = len(training_data.class_indices)\n",
    "        model = define_model(input_shape, num_classes)\n",
    "        steps_per_epoch = training_data.samples // training_data.batch_size\n",
    "        model.fit_generator(training_data,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=self.epochs,\n",
    "                            verbose=2)\n",
    "        model.save(self.output().path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.6: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last task evaluates our model and - if it surpasses the baseline accuracy - saves the evaluation results to the filesystem. Let the task crash if the model does not perform well enough. It's worth an exception!\n",
    "\n",
    "*Input*: ExtractDataset, Configure, TrainModel, BaselineValidation<br>\n",
    "*Output*: A JSON file containing the evaluation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Task):\n",
    "    \n",
    "    dataset_version = IntParameter(default=1)\n",
    "    dataset_name = Parameter(default=\"dataset\")\n",
    "    config_name = Parameter(default=\"standard\")\n",
    "    model_version = IntParameter(default=1)\n",
    "    model_name = Parameter(default=\"keras_model\")\n",
    "\n",
    "    validation_set = \"Test\"\n",
    "\n",
    "    def requires(self):\n",
    "        yield TrainModel(self.dataset_version, \n",
    "                         self.dataset_name, \n",
    "                         self.config_name,\n",
    "                         self.model_version,\n",
    "                         self.model_name)\n",
    "        yield BaselineValidation(self.dataset_version,\n",
    "                                 self.dataset_name,\n",
    "                                 self.config_name)\n",
    "        yield ExtractDataset(self.dataset_version, \n",
    "                             self.dataset_name)\n",
    "        yield Configure(self.config_name)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"evaluation/%d/%s.json\" % (self.model_version, self.model_name))\n",
    "\n",
    "    def run(self):\n",
    "        from tensorflow import keras\n",
    "        self.output().makedirs()\n",
    "        model_path = self.input()[0].path\n",
    "        model = keras.models.load_model(model_path)\n",
    "        dataset = self.input()[2].path\n",
    "        config = self.input()[3].path\n",
    "        test_data = build_generator(config, dataset, self.validation_set)\n",
    "        evaluation = model.evaluate_generator(test_data)\n",
    "\n",
    "        with self.input()[1].open(\"r\") as i:\n",
    "            baseline_acc = json.load(i)[\"acc\"]\n",
    "        acc = evaluation[1]\n",
    "        if acc > baseline_acc:\n",
    "            result = {\"acc\": acc, \"baseline_acc\": baseline_acc}\n",
    "            with self.output().open(\"w\") as o:\n",
    "                json.dump(result, o)\n",
    "        else:\n",
    "            raise Exception(\"Acc %f is smaller than baseline acc %f!\" % (acc, baseline_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise Task No.7: Deploy to TensorFlow-Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras model is performing well. Let's deploy it to TensorFlow Serving.\n",
    "\n",
    "It can be loaded with TensorFlow Serving by the following command:\n",
    "tensorflow_model_server --model_name=\"keras_model\" --model_base_path=\"serving/keras_model\"\n",
    "\n",
    "*Input*: TrainModel, Evaluate </br>\n",
    "*Output*: The TensorFlow-Graph and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(Task):\n",
    "    dataset_version = IntParameter(default=1)\n",
    "    dataset_name = Parameter(default=\"dataset\")\n",
    "    config_name = Parameter(default=\"standard\")\n",
    "    model_version = IntParameter(default=1)\n",
    "    model_name = Parameter(default=\"keras_model\")\n",
    "\n",
    "    def requires(self):\n",
    "        yield Evaluate(self.dataset_version,\n",
    "                       self.dataset_name,\n",
    "                       self.config_name,\n",
    "                       self.model_version,\n",
    "                       self.model_name)\n",
    "        yield TrainModel(self.dataset_version,\n",
    "                         self.dataset_name,\n",
    "                         self.config_name,\n",
    "                         self.model_version,\n",
    "                         self.model_name)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"serving/%s/%d\" % (self.model_name,\n",
    "                                              self.model_version))\n",
    "\n",
    "    def run(self):\n",
    "        from tensorflow import keras\n",
    "        self.output().makedirs()\n",
    "        model_path = self.input()[1].path\n",
    "        model = keras.models.load_model(model_path)\n",
    "        tensor_info_input = tf.saved_model.utils.build_tensor_info(model.input)\n",
    "        tensor_info_output = tf.saved_model.utils.build_tensor_info(model.output)\n",
    "        prediction_signature = (\n",
    "            tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                inputs={'input': tensor_info_input},\n",
    "                outputs={'prediction': tensor_info_output},\n",
    "                method_name=signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "        export_path = self.output().path\n",
    "        tf_builder = builder.SavedModelBuilder(export_path)\n",
    "        with tf.keras.backend.get_session() as sess:\n",
    "            tf_builder.add_meta_graph_and_variables(\n",
    "                sess=sess,\n",
    "                tags=[tag_constants.SERVING],\n",
    "                signature_def_map={\n",
    "                    signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_signature\n",
    "                }\n",
    "            )\n",
    "            tf_builder.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
